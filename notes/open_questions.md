

1. for the agent evaluators: which agents would we concentrate on. the ones with best results could be limited
   - either GPU access needed (google collap pro)
   - API access (paid): 

   OR datasets needed with run evaluations: no published datasets found (lookup in some agent repos needed)
    - running agents on smaller models
    - 

2. question: which benchmark (SWE-bench Verified / SWE-bench)

paper writing - tips:
first full implementation, then descriptions

literature: 
i read some papers about swe benchmarks, agents etc. do you see some neccessity in reading some other specifics 

data exists 
verified: scripts
pr arena: pull from the github sparkle

swe benhc verified results and maybe even for pr arena

